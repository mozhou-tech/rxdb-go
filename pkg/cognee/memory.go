package cognee

import (
	"context"
	"fmt"
	"sort"
	"time"

	"github.com/mozhou-tech/rxdb-go/pkg/rxdb"
	"github.com/sirupsen/logrus"
)

// MemoryService è®°å¿†æœåŠ¡ï¼Œæä¾›ç±»ä¼¼ Cognee çš„ AI è®°å¿†åŠŸèƒ½
type MemoryService struct {
	db           rxdb.Database
	memories     rxdb.Collection // è®°å¿†æ•°æ®é›†åˆ
	chunks       rxdb.Collection // æ–‡æœ¬å—é›†åˆ
	entities     rxdb.Collection // å®žä½“é›†åˆ
	relations    rxdb.Collection // å…³ç³»é›†åˆ
	fulltext     *rxdb.FulltextSearch
	vectorSearch *rxdb.VectorSearch
	graphDB      rxdb.GraphDatabase
	embedder     Embedder                // å‘é‡åµŒå…¥ç”Ÿæˆå™¨
	extractor    EntityRelationExtractor // å®žä½“å…³ç³»æŠ½å–å™¨
	// æ··åˆæœç´¢æƒé‡ï¼šfulltextWeight + vectorWeight åº”è¯¥ç­‰äºŽ 1.0
	fulltextWeight float64 // å…¨æ–‡æœç´¢æƒé‡ï¼Œé»˜è®¤ 0.7
	vectorWeight   float64 // å‘é‡æœç´¢æƒé‡ï¼Œé»˜è®¤ 0.3
}

// Embedder å‘é‡åµŒå…¥ç”Ÿæˆå™¨æŽ¥å£
type Embedder interface {
	// Embed å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡åµŒå…¥
	Embed(ctx context.Context, text string) ([]float64, error)
	// Dimensions è¿”å›žå‘é‡ç»´åº¦
	Dimensions() int
}

// Memory è®°å¿†æ•°æ®ç»“æž„
type Memory struct {
	ID          string                 `json:"id"`
	Content     string                 `json:"content"`
	Type        string                 `json:"type"` // text, code, url, etc.
	Dataset     string                 `json:"dataset"`
	Metadata    map[string]interface{} `json:"metadata"`
	CreatedAt   int64                  `json:"created_at"`
	ProcessedAt int64                  `json:"processed_at"`
	Chunks      []string               `json:"chunks"` // å…³è”çš„æ–‡æœ¬å— ID
}

// Chunk æ–‡æœ¬å—ç»“æž„
type Chunk struct {
	ID        string                 `json:"id"`
	MemoryID  string                 `json:"memory_id"`
	Content   string                 `json:"content"`
	Index     int                    `json:"index"`
	Metadata  map[string]interface{} `json:"metadata"`
	CreatedAt int64                  `json:"created_at"`
}

// Entity å®žä½“ç»“æž„
type Entity struct {
	ID        string                 `json:"id"`
	Name      string                 `json:"name"`
	Type      string                 `json:"type"` // person, organization, concept, etc.
	Metadata  map[string]interface{} `json:"metadata"`
	CreatedAt int64                  `json:"created_at"`
}

// Relation å…³ç³»ç»“æž„
type Relation struct {
	ID        string                 `json:"id"`
	From      string                 `json:"from"` // æºå®žä½“ ID
	To        string                 `json:"to"`   // ç›®æ ‡å®žä½“ ID
	Type      string                 `json:"type"` // å…³ç³»ç±»åž‹
	Metadata  map[string]interface{} `json:"metadata"`
	CreatedAt int64                  `json:"created_at"`
}

// Dataset æ•°æ®é›†ç»“æž„
type Dataset struct {
	ID          string                 `json:"id"`
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Metadata    map[string]interface{} `json:"metadata"`
	CreatedAt   int64                  `json:"created_at"`
	Status      string                 `json:"status"` // pending, processing, completed, error
}

// MemoryServiceOptions è®°å¿†æœåŠ¡é…ç½®é€‰é¡¹
type MemoryServiceOptions struct {
	// Embedder å‘é‡åµŒå…¥ç”Ÿæˆå™¨ï¼ˆå¿…éœ€ï¼‰
	Embedder Embedder
	// Extractor å®žä½“å…³ç³»æŠ½å–å™¨ï¼ˆå¯é€‰ï¼Œå¦‚æžœä¸æä¾›åˆ™ä¸è¿›è¡Œå®žä½“å…³ç³»æŠ½å–ï¼‰
	Extractor EntityRelationExtractor
	// FulltextIndexOptions å…¨æ–‡æœç´¢ç´¢å¼•é€‰é¡¹
	FulltextIndexOptions *rxdb.FulltextIndexOptions
	// VectorSearchOptions å‘é‡æœç´¢é€‰é¡¹
	VectorSearchOptions *VectorSearchOptions
	// HybridSearchWeights æ··åˆæœç´¢æƒé‡é…ç½®
	// FulltextWeight: å…¨æ–‡æœç´¢æƒé‡ï¼Œé»˜è®¤ 0.7
	// VectorWeight: å‘é‡æœç´¢æƒé‡ï¼Œé»˜è®¤ 0.3
	// ä¸¤è€…ä¹‹å’Œåº”è¯¥ç­‰äºŽ 1.0
	HybridSearchWeights *HybridSearchWeights
}

// HybridSearchWeights æ··åˆæœç´¢æƒé‡é…ç½®
type HybridSearchWeights struct {
	FulltextWeight float64 // å…¨æ–‡æœç´¢æƒé‡ï¼Œé»˜è®¤ 0.7
	VectorWeight   float64 // å‘é‡æœç´¢æƒé‡ï¼Œé»˜è®¤ 0.3
}

// VectorSearchOptions å‘é‡æœç´¢é€‰é¡¹
type VectorSearchOptions struct {
	DistanceMetric string // cosine, euclidean, dot
	IndexType      string // flat, ivf
}

// NewMemoryService åˆ›å»ºæ–°çš„è®°å¿†æœåŠ¡
func NewMemoryService(ctx context.Context, db rxdb.Database, opts MemoryServiceOptions) (*MemoryService, error) {
	logrus.WithFields(logrus.Fields{
		"hasEmbedder":          opts.Embedder != nil,
		"hasFulltextIndexOpts": opts.FulltextIndexOptions != nil,
		"hasVectorSearchOpts":  opts.VectorSearchOptions != nil,
	}).Info("ðŸ”§ NewMemoryService: å¼€å§‹åˆ›å»ºè®°å¿†æœåŠ¡")

	if opts.Embedder == nil {
		logrus.Error("âŒ NewMemoryService: embedder is required")
		return nil, fmt.Errorf("embedder is required")
	}

	// åˆ›å»ºæˆ–èŽ·å–é›†åˆ
	logrus.Debug("ðŸ“¦ NewMemoryService: åˆ›å»º memories é›†åˆ")
	memoriesSchema := rxdb.Schema{
		PrimaryKey: "id",
		RevField:   "_rev",
	}
	memories, err := db.Collection(ctx, "memories", memoriesSchema)
	if err != nil {
		logrus.WithError(err).Error("âŒ NewMemoryService: åˆ›å»º memories é›†åˆå¤±è´¥")
		return nil, fmt.Errorf("failed to create memories collection: %w", err)
	}
	logrus.Debug("âœ… NewMemoryService: memories é›†åˆåˆ›å»ºæˆåŠŸ")

	logrus.Debug("ðŸ“¦ NewMemoryService: åˆ›å»º chunks é›†åˆ")
	chunksSchema := rxdb.Schema{
		PrimaryKey: "id",
		RevField:   "_rev",
	}
	chunks, err := db.Collection(ctx, "chunks", chunksSchema)
	if err != nil {
		logrus.WithError(err).Error("âŒ NewMemoryService: åˆ›å»º chunks é›†åˆå¤±è´¥")
		return nil, fmt.Errorf("failed to create chunks collection: %w", err)
	}
	logrus.Debug("âœ… NewMemoryService: chunks é›†åˆåˆ›å»ºæˆåŠŸ")

	logrus.Debug("ðŸ“¦ NewMemoryService: åˆ›å»º entities é›†åˆ")
	entitiesSchema := rxdb.Schema{
		PrimaryKey: "id",
		RevField:   "_rev",
	}
	entities, err := db.Collection(ctx, "entities", entitiesSchema)
	if err != nil {
		logrus.WithError(err).Error("âŒ NewMemoryService: åˆ›å»º entities é›†åˆå¤±è´¥")
		return nil, fmt.Errorf("failed to create entities collection: %w", err)
	}
	logrus.Debug("âœ… NewMemoryService: entities é›†åˆåˆ›å»ºæˆåŠŸ")

	logrus.Debug("ðŸ“¦ NewMemoryService: åˆ›å»º relations é›†åˆ")
	relationsSchema := rxdb.Schema{
		PrimaryKey: "id",
		RevField:   "_rev",
	}
	relations, err := db.Collection(ctx, "relations", relationsSchema)
	if err != nil {
		logrus.WithError(err).Error("âŒ NewMemoryService: åˆ›å»º relations é›†åˆå¤±è´¥")
		return nil, fmt.Errorf("failed to create relations collection: %w", err)
	}
	logrus.Debug("âœ… NewMemoryService: relations é›†åˆåˆ›å»ºæˆåŠŸ")

	// åˆ›å»ºå…¨æ–‡æœç´¢
	// é…ç½®å…¨æ–‡æœç´¢é€‰é¡¹
	logrus.Debug("ðŸ” NewMemoryService: å¼€å§‹é…ç½®å…¨æ–‡æœç´¢é€‰é¡¹")
	fulltextOpts := opts.FulltextIndexOptions
	if fulltextOpts == nil {
		fulltextOpts = &rxdb.FulltextIndexOptions{
			Tokenize:      "jieba",
			CaseSensitive: false,
		}
		logrus.WithFields(logrus.Fields{
			"tokenize":      fulltextOpts.Tokenize,
			"caseSensitive": fulltextOpts.CaseSensitive,
		}).Debug("ðŸ“ NewMemoryService: ä½¿ç”¨é»˜è®¤å…¨æ–‡æœç´¢é€‰é¡¹")
	} else {
		logrus.WithFields(logrus.Fields{
			"tokenize":      fulltextOpts.Tokenize,
			"caseSensitive": fulltextOpts.CaseSensitive,
		}).Debug("ðŸ“ NewMemoryService: ä½¿ç”¨è‡ªå®šä¹‰å…¨æ–‡æœç´¢é€‰é¡¹")
	}

	logrus.Info("ðŸ” NewMemoryService: å¼€å§‹åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨ jieba åˆ†è¯å™¨æ—¶ï¼‰")
	fulltext, err := rxdb.AddFulltextSearch(memories, rxdb.FulltextSearchConfig{
		Identifier: "memories_search",
		DocToString: func(doc map[string]any) string {
			content, _ := doc["content"].(string)
			return content
		},
		IndexOptions:   fulltextOpts,
		Initialization: "instant",
	})
	if err != nil {
		logrus.WithError(err).WithFields(logrus.Fields{
			"tokenize":      fulltextOpts.Tokenize,
			"caseSensitive": fulltextOpts.CaseSensitive,
		}).Error("âŒ NewMemoryService: åˆ›å»ºå…¨æ–‡æœç´¢å¤±è´¥")
		return nil, fmt.Errorf("failed to create fulltext search: %w", err)
	}
	logrus.Info("âœ… NewMemoryService: å…¨æ–‡æœç´¢ç´¢å¼•åˆ›å»ºæˆåŠŸ")

	// é…ç½®å‘é‡æœç´¢é€‰é¡¹
	logrus.Debug("ðŸ”¢ NewMemoryService: å¼€å§‹é…ç½®å‘é‡æœç´¢é€‰é¡¹")
	distanceMetric := "cosine"
	if opts.VectorSearchOptions != nil && opts.VectorSearchOptions.DistanceMetric != "" {
		distanceMetric = opts.VectorSearchOptions.DistanceMetric
		logrus.WithField("distanceMetric", distanceMetric).Debug("ðŸ“ NewMemoryService: ä½¿ç”¨è‡ªå®šä¹‰å‘é‡æœç´¢é€‰é¡¹")
	} else {
		logrus.WithField("distanceMetric", distanceMetric).Debug("ðŸ“ NewMemoryService: ä½¿ç”¨é»˜è®¤å‘é‡æœç´¢é€‰é¡¹")
	}

	// åˆ›å»ºå‘é‡æœç´¢
	logrus.WithFields(logrus.Fields{
		"dimensions":     opts.Embedder.Dimensions(),
		"distanceMetric": distanceMetric,
	}).Info("ðŸ”¢ NewMemoryService: å¼€å§‹åˆ›å»ºå‘é‡æœç´¢ç´¢å¼•")
	vectorSearch, err := rxdb.AddVectorSearch(memories, rxdb.VectorSearchConfig{
		Identifier: "memories_vector",
		DocToEmbedding: func(doc map[string]any) ([]float64, error) {
			content, _ := doc["content"].(string)
			if content == "" {
				return nil, fmt.Errorf("empty content")
			}
			return opts.Embedder.Embed(ctx, content)
		},
		Dimensions:     opts.Embedder.Dimensions(),
		DistanceMetric: distanceMetric,
		Initialization: "instant",
	})
	if err != nil {
		logrus.WithError(err).WithFields(logrus.Fields{
			"dimensions":     opts.Embedder.Dimensions(),
			"distanceMetric": distanceMetric,
		}).Error("âŒ NewMemoryService: åˆ›å»ºå‘é‡æœç´¢å¤±è´¥")
		return nil, fmt.Errorf("failed to create vector search: %w", err)
	}
	logrus.Info("âœ… NewMemoryService: å‘é‡æœç´¢ç´¢å¼•åˆ›å»ºæˆåŠŸ")

	// èŽ·å–å›¾æ•°æ®åº“
	logrus.Debug("ðŸ•¸ï¸  NewMemoryService: èŽ·å–å›¾æ•°æ®åº“")
	graphDB := db.Graph()
	if graphDB != nil {
		logrus.Debug("âœ… NewMemoryService: å›¾æ•°æ®åº“å·²èŽ·å–")
	} else {
		logrus.Warn("âš ï¸  NewMemoryService: å›¾æ•°æ®åº“ä¸å¯ç”¨")
	}

	// é…ç½®æ··åˆæœç´¢æƒé‡
	fulltextWeight := 0.7 // é»˜è®¤å…¨æ–‡æœç´¢æƒé‡
	vectorWeight := 0.3   // é»˜è®¤å‘é‡æœç´¢æƒé‡
	if opts.HybridSearchWeights != nil {
		if opts.HybridSearchWeights.FulltextWeight > 0 {
			fulltextWeight = opts.HybridSearchWeights.FulltextWeight
		}
		if opts.HybridSearchWeights.VectorWeight > 0 {
			vectorWeight = opts.HybridSearchWeights.VectorWeight
		}
		// å½’ä¸€åŒ–æƒé‡ï¼Œç¡®ä¿æ€»å’Œä¸º 1.0
		totalWeight := fulltextWeight + vectorWeight
		if totalWeight > 0 {
			fulltextWeight = fulltextWeight / totalWeight
			vectorWeight = vectorWeight / totalWeight
		}
	}
	logrus.WithFields(logrus.Fields{
		"fulltextWeight": fulltextWeight,
		"vectorWeight":   vectorWeight,
	}).Debug("âš–ï¸  NewMemoryService: æ··åˆæœç´¢æƒé‡é…ç½®")

	// å¦‚æžœæ²¡æœ‰æä¾›æŠ½å–å™¨ï¼Œä½¿ç”¨ç©ºæ“ä½œæŠ½å–å™¨
	extractor := opts.Extractor
	if extractor == nil {
		extractor = &NoOpExtractor{}
		logrus.Debug("ðŸ“ NewMemoryService: æœªæä¾›æŠ½å–å™¨ï¼Œä½¿ç”¨ç©ºæ“ä½œæŠ½å–å™¨")
	} else {
		logrus.Debug("âœ… NewMemoryService: å·²é…ç½®å®žä½“å…³ç³»æŠ½å–å™¨")
	}

	service := &MemoryService{
		db:             db,
		memories:       memories,
		chunks:         chunks,
		entities:       entities,
		relations:      relations,
		fulltext:       fulltext,
		vectorSearch:   vectorSearch,
		graphDB:        graphDB,
		embedder:       opts.Embedder,
		extractor:      extractor,
		fulltextWeight: fulltextWeight,
		vectorWeight:   vectorWeight,
	}

	logrus.Info("âœ… NewMemoryService: è®°å¿†æœåŠ¡åˆ›å»ºæˆåŠŸ")
	return service, nil
}

// AddMemory æ·»åŠ è®°å¿†æ•°æ®
func (s *MemoryService) AddMemory(ctx context.Context, content string, memoryType string, dataset string, metadata map[string]interface{}) (*Memory, error) {
	now := time.Now().Unix()

	memory := Memory{
		ID:        generateID(),
		Content:   content,
		Type:      memoryType,
		Dataset:   dataset,
		Metadata:  metadata,
		CreatedAt: now,
		Chunks:    []string{},
	}

	doc := map[string]any{
		"id":         memory.ID,
		"content":    memory.Content,
		"type":       memory.Type,
		"dataset":    memory.Dataset,
		"metadata":   memory.Metadata,
		"created_at": memory.CreatedAt,
		"chunks":     memory.Chunks,
	}

	docObj, err := s.memories.Insert(ctx, doc)
	if err != nil {
		return nil, fmt.Errorf("failed to insert memory: %w", err)
	}

	memory.ID = docObj.ID()
	return &memory, nil
}

// ProcessMemory å¤„ç†è®°å¿†æ•°æ®ï¼Œæå–å®žä½“å’Œå…³ç³»
func (s *MemoryService) ProcessMemory(ctx context.Context, memoryID string) error {
	// èŽ·å–è®°å¿†
	memoryDoc, err := s.memories.FindByID(ctx, memoryID)
	if err != nil {
		return fmt.Errorf("memory not found: %w", err)
	}

	memoryData := memoryDoc.Data()
	content, _ := memoryData["content"].(string)

	// ä½¿ç”¨æŠ½å–å™¨æå–å®žä½“å’Œå…³ç³»
	var entities []Entity
	var relations []Relation

	if s.extractor != nil {
		var extractErr error
		entities, extractErr = s.extractor.ExtractEntities(ctx, content)
		if extractErr != nil {
			logrus.WithError(extractErr).WithField("memory_id", memoryID).Warn("Failed to extract entities, continuing without entities")
			entities = []Entity{}
		}

		relations, extractErr = s.extractor.ExtractRelations(ctx, content, entities)
		if extractErr != nil {
			logrus.WithError(extractErr).WithField("memory_id", memoryID).Warn("Failed to extract relations, continuing without relations")
			relations = []Relation{}
		}
	} else {
		// å¦‚æžœæ²¡æœ‰é…ç½®æŠ½å–å™¨ï¼Œä½¿ç”¨æ—§çš„ç®€å•æå–æ–¹æ³•ï¼ˆå‘åŽå…¼å®¹ï¼‰
		entities = extractEntities(content)
		relations = extractRelations(content, entities)
	}

	// ä¿å­˜å®žä½“
	for _, entity := range entities {
		entityDoc := map[string]any{
			"id":         entity.ID,
			"name":       entity.Name,
			"type":       entity.Type,
			"metadata":   entity.Metadata,
			"created_at": entity.CreatedAt,
		}
		_, err := s.entities.Upsert(ctx, entityDoc)
		if err != nil {
			return fmt.Errorf("failed to upsert entity: %w", err)
		}

		// åœ¨å›¾æ•°æ®åº“ä¸­åˆ›å»ºèŠ‚ç‚¹
		if s.graphDB != nil {
			nodeID := fmt.Sprintf("entity:%s", entity.ID)
			_ = s.graphDB.Link(ctx, nodeID, "type", entity.Type)
		}
	}

	// ä¿å­˜å…³ç³»
	for _, relation := range relations {
		relationDoc := map[string]any{
			"id":         relation.ID,
			"from":       relation.From,
			"to":         relation.To,
			"type":       relation.Type,
			"metadata":   relation.Metadata,
			"created_at": relation.CreatedAt,
		}
		_, err := s.relations.Upsert(ctx, relationDoc)
		if err != nil {
			return fmt.Errorf("failed to upsert relation: %w", err)
		}

		// åœ¨å›¾æ•°æ®åº“ä¸­åˆ›å»ºé“¾æŽ¥
		if s.graphDB != nil {
			fromNode := fmt.Sprintf("entity:%s", relation.From)
			toNode := fmt.Sprintf("entity:%s", relation.To)
			_ = s.graphDB.Link(ctx, fromNode, relation.Type, toNode)
		}
	}

	// æ›´æ–°è®°å¿†çš„å¤„ç†çŠ¶æ€
	now := time.Now().Unix()
	memoryData["processed_at"] = now
	_, err = s.memories.Upsert(ctx, memoryData)
	if err != nil {
		return fmt.Errorf("failed to update memory: %w", err)
	}

	return nil
}

// Search æœç´¢è®°å¿†
func (s *MemoryService) Search(ctx context.Context, query string, searchType string, limit int) ([]SearchResult, error) {
	switch searchType {
	case "CHUNKS", "FULLTEXT":
		return s.searchFulltext(ctx, query, limit)
	case "VECTOR", "SEMANTIC":
		return s.searchVector(ctx, query, limit)
	case "GRAPH", "INSIGHTS":
		return s.searchGraph(ctx, query, limit)
	case "HYBRID":
		return s.searchHybrid(ctx, query, limit)
	default:
		return s.searchHybrid(ctx, query, limit)
	}
}

// searchFulltext å…¨æ–‡æœç´¢
func (s *MemoryService) searchFulltext(ctx context.Context, query string, limit int) ([]SearchResult, error) {
	results, err := s.fulltext.FindWithScores(ctx, query, rxdb.FulltextSearchOptions{
		Limit: limit,
	})
	if err != nil {
		return nil, fmt.Errorf("fulltext search failed: %w", err)
	}

	searchResults := make([]SearchResult, len(results))
	for i, r := range results {
		data := r.Document.Data()
		searchResults[i] = SearchResult{
			ID:      r.Document.ID(),
			Content: getString(data, "content"),
			Type:    getString(data, "type"),
			Score:   r.Score,
			Source:  "fulltext",
		}
	}

	return searchResults, nil
}

// searchVector å‘é‡æœç´¢
func (s *MemoryService) searchVector(ctx context.Context, query string, limit int) ([]SearchResult, error) {
	// ç”ŸæˆæŸ¥è¯¢å‘é‡
	queryEmbedding, err := s.embedder.Embed(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("failed to generate embedding: %w", err)
	}

	results, err := s.vectorSearch.Search(ctx, queryEmbedding, rxdb.VectorSearchOptions{
		Limit: limit,
	})
	if err != nil {
		return nil, fmt.Errorf("vector search failed: %w", err)
	}

	searchResults := make([]SearchResult, len(results))
	for i, r := range results {
		data := r.Document.Data()
		searchResults[i] = SearchResult{
			ID:       r.Document.ID(),
			Content:  getString(data, "content"),
			Type:     getString(data, "type"),
			Score:    r.Score,
			Distance: r.Distance,
			Source:   "vector",
		}
	}

	return searchResults, nil
}

// searchGraph å›¾æœç´¢
func (s *MemoryService) searchGraph(ctx context.Context, query string, limit int) ([]SearchResult, error) {
	if s.graphDB == nil {
		return []SearchResult{}, nil
	}

	// ä½¿ç”¨å…¨æ–‡æœç´¢æ‰¾åˆ°ç›¸å…³å®žä½“
	entityResults, err := s.searchFulltext(ctx, query, 10)
	if err != nil {
		return nil, err
	}

	// ä»Žå›¾æ•°æ®åº“ä¸­æ‰¾åˆ°ç›¸å…³èŠ‚ç‚¹
	var searchResults []SearchResult
	seen := make(map[string]bool)

	for _, entityResult := range entityResults {
		nodeID := fmt.Sprintf("entity:%s", entityResult.ID)
		neighbors, err := s.graphDB.GetNeighbors(ctx, nodeID, "")
		if err == nil {
			for _, neighbor := range neighbors {
				if !seen[neighbor] {
					seen[neighbor] = true
					searchResults = append(searchResults, SearchResult{
						ID:     neighbor,
						Source: "graph",
						Score:  0.8, // å›¾æœç´¢çš„é»˜è®¤åˆ†æ•°
					})
				}
			}
		}
	}

	if len(searchResults) > limit {
		searchResults = searchResults[:limit]
	}

	return searchResults, nil
}

// searchHybrid æ··åˆæœç´¢ï¼ˆç»“åˆå…¨æ–‡å’Œå‘é‡ï¼‰
func (s *MemoryService) searchHybrid(ctx context.Context, query string, limit int) ([]SearchResult, error) {
	// å¹¶è¡Œæ‰§è¡Œå…¨æ–‡å’Œå‘é‡æœç´¢
	fulltextResults, _ := s.searchFulltext(ctx, query, limit)
	vectorResults, _ := s.searchVector(ctx, query, limit)

	// åˆ›å»ºä¸¤ä¸ªæ˜ å°„æ¥å­˜å‚¨åŽŸå§‹åˆ†æ•°
	fulltextScores := make(map[string]float64)
	vectorScores := make(map[string]float64)
	resultMap := make(map[string]*SearchResult)

	// æ”¶é›†å…¨æ–‡æœç´¢ç»“æžœ
	for _, r := range fulltextResults {
		fulltextScores[r.ID] = r.Score
		if _, ok := resultMap[r.ID]; !ok {
			resultMap[r.ID] = &SearchResult{
				ID:      r.ID,
				Content: r.Content,
				Type:    r.Type,
				Source:  "hybrid",
			}
		}
	}

	// æ”¶é›†å‘é‡æœç´¢ç»“æžœ
	for _, r := range vectorResults {
		vectorScores[r.ID] = r.Score
		if existing, ok := resultMap[r.ID]; ok {
			existing.Distance = r.Distance
		} else {
			resultMap[r.ID] = &SearchResult{
				ID:       r.ID,
				Content:  r.Content,
				Type:     r.Type,
				Distance: r.Distance,
				Source:   "hybrid",
			}
		}
	}

	// è®¡ç®—åŠ æƒå¹³å‡åˆ†æ•°
	for id, result := range resultMap {
		var finalScore float64
		fulltextScore, hasFulltext := fulltextScores[id]
		vectorScore, hasVector := vectorScores[id]

		if hasFulltext && hasVector {
			// åŒæ—¶å‡ºçŽ°åœ¨ä¸¤ä¸ªç»“æžœä¸­ï¼Œä½¿ç”¨åŠ æƒå¹³å‡
			finalScore = fulltextScore*s.fulltextWeight + vectorScore*s.vectorWeight
		} else if hasFulltext {
			// åªå‡ºçŽ°åœ¨å…¨æ–‡æœç´¢ç»“æžœä¸­
			finalScore = fulltextScore * s.fulltextWeight
		} else if hasVector {
			// åªå‡ºçŽ°åœ¨å‘é‡æœç´¢ç»“æžœä¸­
			finalScore = vectorScore * s.vectorWeight
		}

		result.Score = finalScore
	}

	// è½¬æ¢ä¸ºåˆ‡ç‰‡å¹¶æŽ’åº
	results := make([]SearchResult, 0, len(resultMap))
	for _, r := range resultMap {
		results = append(results, *r)
	}

	// æŒ‰åˆ†æ•°æŽ’åº
	sortResultsByScore(results)

	if len(results) > limit {
		results = results[:limit]
	}

	return results, nil
}

// SearchResult æœç´¢ç»“æžœ
type SearchResult struct {
	ID       string  `json:"id"`
	Content  string  `json:"content"`
	Type     string  `json:"type"`
	Score    float64 `json:"score"`
	Distance float64 `json:"distance,omitempty"`
	Source   string  `json:"source"` // fulltext, vector, graph, hybrid
}

// è¾…åŠ©å‡½æ•°
func generateID() string {
	return fmt.Sprintf("%d", time.Now().UnixNano())
}

func getString(data map[string]any, key string) string {
	if val, ok := data[key].(string); ok {
		return val
	}
	return ""
}

func extractEntities(content string) []Entity {
	// ç®€å•çš„å®žä½“æå–ï¼ˆå®žé™…åº”è¯¥ä½¿ç”¨ NLP æ¨¡åž‹ï¼‰
	// è¿™é‡Œè¿”å›žç©ºåˆ—è¡¨ä½œä¸ºç¤ºä¾‹
	return []Entity{}
}

func extractRelations(content string, entities []Entity) []Relation {
	// ç®€å•çš„å…³ç³»æå–ï¼ˆå®žé™…åº”è¯¥ä½¿ç”¨ NLP æ¨¡åž‹ï¼‰
	// è¿™é‡Œè¿”å›žç©ºåˆ—è¡¨ä½œä¸ºç¤ºä¾‹
	return []Relation{}
}

func sortResultsByScore(results []SearchResult) {
	// æŒ‰åˆ†æ•°é™åºæŽ’åº
	sort.Slice(results, func(i, j int) bool {
		return results[i].Score > results[j].Score
	})
}

// ProcessDataset å¤„ç†æ•´ä¸ªæ•°æ®é›†
func (s *MemoryService) ProcessDataset(ctx context.Context, datasetID string) (int, error) {
	query := s.memories.Find(map[string]any{"dataset": datasetID})
	docs, err := query.Exec(ctx)
	if err != nil {
		return 0, fmt.Errorf("failed to query memories: %w", err)
	}

	count := 0
	for _, doc := range docs {
		if err := s.ProcessMemory(ctx, doc.ID()); err == nil {
			count++
		}
	}

	return count, nil
}

// DeleteMemory åˆ é™¤è®°å¿†
func (s *MemoryService) DeleteMemory(ctx context.Context, memoryID string) error {
	return s.memories.Remove(ctx, memoryID)
}

// DeleteDataset åˆ é™¤æ•´ä¸ªæ•°æ®é›†
func (s *MemoryService) DeleteDataset(ctx context.Context, datasetID string) error {
	query := s.memories.Find(map[string]any{"dataset": datasetID})
	docs, err := query.Exec(ctx)
	if err != nil {
		return fmt.Errorf("failed to query memories: %w", err)
	}

	for _, doc := range docs {
		if err := s.memories.Remove(ctx, doc.ID()); err != nil {
			return fmt.Errorf("failed to remove memory %s: %w", doc.ID(), err)
		}
	}

	return nil
}

// ListDatasets åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
func (s *MemoryService) ListDatasets(ctx context.Context) ([]*Dataset, error) {
	allDocs, err := s.memories.All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get all memories: %w", err)
	}

	datasetMap := make(map[string]*Dataset)
	for _, doc := range allDocs {
		data := doc.Data()
		datasetName, _ := data["dataset"].(string)
		if datasetName == "" {
			datasetName = "main_dataset"
		}

		if _, exists := datasetMap[datasetName]; !exists {
			createdAt, _ := data["created_at"].(int64)
			datasetMap[datasetName] = &Dataset{
				ID:        datasetName,
				Name:      datasetName,
				CreatedAt: createdAt,
				Status:    "completed",
			}
		}
	}

	datasets := make([]*Dataset, 0, len(datasetMap))
	for _, dataset := range datasetMap {
		datasets = append(datasets, dataset)
	}

	return datasets, nil
}

// GetDatasetData èŽ·å–æ•°æ®é›†çš„æ•°æ®
func (s *MemoryService) GetDatasetData(ctx context.Context, datasetID string) ([]map[string]interface{}, error) {
	query := s.memories.Find(map[string]any{"dataset": datasetID})
	docs, err := query.Exec(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to query memories: %w", err)
	}

	data := make([]map[string]interface{}, len(docs))
	for i, doc := range docs {
		data[i] = doc.Data()
	}

	return data, nil
}

// GetDatasetStatus èŽ·å–æ•°æ®é›†çŠ¶æ€
func (s *MemoryService) GetDatasetStatus(ctx context.Context, datasetID string) (*DatasetStatus, error) {
	query := s.memories.Find(map[string]any{"dataset": datasetID})
	docs, err := query.Exec(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to query memories: %w", err)
	}

	processedCount := 0
	for _, doc := range docs {
		data := doc.Data()
		if processedAt, ok := data["processed_at"].(int64); ok && processedAt > 0 {
			processedCount++
		}
	}

	status := "completed"
	if processedCount < len(docs) {
		status = "processing"
	}

	return &DatasetStatus{
		Dataset:   datasetID,
		Status:    status,
		Total:     len(docs),
		Processed: processedCount,
		Pending:   len(docs) - processedCount,
	}, nil
}

// DatasetStatus æ•°æ®é›†çŠ¶æ€
type DatasetStatus struct {
	Dataset   string `json:"dataset"`
	Status    string `json:"status"`
	Total     int    `json:"total"`
	Processed int    `json:"processed"`
	Pending   int    `json:"pending"`
}

// GetGraphData èŽ·å–å›¾è°±æ•°æ®ç”¨äºŽå¯è§†åŒ–
func (s *MemoryService) GetGraphData(ctx context.Context) (*GraphData, error) {
	// èŽ·å–æ‰€æœ‰å®žä½“
	allEntities, err := s.entities.All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get entities: %w", err)
	}

	// èŽ·å–æ‰€æœ‰å…³ç³»
	allRelations, err := s.relations.All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get relations: %w", err)
	}

	nodes := make([]GraphNode, len(allEntities))
	for i, entity := range allEntities {
		data := entity.Data()
		nodes[i] = GraphNode{
			ID:   entity.ID(),
			Name: getString(data, "name"),
			Type: getString(data, "type"),
		}
	}

	edges := make([]GraphEdge, len(allRelations))
	for i, relation := range allRelations {
		data := relation.Data()
		edges[i] = GraphEdge{
			From: getString(data, "from"),
			To:   getString(data, "to"),
			Type: getString(data, "type"),
		}
	}

	return &GraphData{
		Nodes: nodes,
		Edges: edges,
	}, nil
}

// GraphData å›¾è°±æ•°æ®
type GraphData struct {
	Nodes []GraphNode `json:"nodes"`
	Edges []GraphEdge `json:"edges"`
}

// GraphNode å›¾è°±èŠ‚ç‚¹
type GraphNode struct {
	ID   string `json:"id"`
	Name string `json:"name"`
	Type string `json:"type"`
}

// GraphEdge å›¾è°±è¾¹
type GraphEdge struct {
	From string `json:"from"`
	To   string `json:"to"`
	Type string `json:"type"`
}

// GetMemory èŽ·å–è®°å¿†
func (s *MemoryService) GetMemory(ctx context.Context, memoryID string) (*Memory, error) {
	doc, err := s.memories.FindByID(ctx, memoryID)
	if err != nil {
		return nil, fmt.Errorf("memory not found: %w", err)
	}

	data := doc.Data()
	memory := &Memory{
		ID:          doc.ID(),
		Content:     getString(data, "content"),
		Type:        getString(data, "type"),
		Dataset:     getString(data, "dataset"),
		Metadata:    getMap(data, "metadata"),
		CreatedAt:   getInt64(data, "created_at"),
		ProcessedAt: getInt64(data, "processed_at"),
	}

	if chunks, ok := data["chunks"].([]interface{}); ok {
		memory.Chunks = make([]string, len(chunks))
		for i, chunk := range chunks {
			if chunkStr, ok := chunk.(string); ok {
				memory.Chunks[i] = chunkStr
			}
		}
	}

	return memory, nil
}

// Health å¥åº·æ£€æŸ¥
func (s *MemoryService) Health(ctx context.Context) (*HealthStatus, error) {
	memoryCount, _ := s.memories.Count(ctx)
	entityCount, _ := s.entities.Count(ctx)
	relationCount, _ := s.relations.Count(ctx)

	return &HealthStatus{
		Status: "healthy",
		Stats: HealthStats{
			Memories:  memoryCount,
			Entities:  entityCount,
			Relations: relationCount,
		},
	}, nil
}

// HealthStatus å¥åº·çŠ¶æ€
type HealthStatus struct {
	Status string      `json:"status"`
	Stats  HealthStats `json:"stats"`
}

// HealthStats å¥åº·ç»Ÿè®¡
type HealthStats struct {
	Memories  int `json:"memories"`
	Entities  int `json:"entities"`
	Relations int `json:"relations"`
}

// è¾…åŠ©å‡½æ•°
func getInt64(data map[string]any, key string) int64 {
	if val, ok := data[key].(int64); ok {
		return val
	}
	if val, ok := data[key].(float64); ok {
		return int64(val)
	}
	return 0
}

func getMap(data map[string]any, key string) map[string]interface{} {
	if val, ok := data[key].(map[string]interface{}); ok {
		return val
	}
	return make(map[string]interface{})
}
